# PandaFactor æ•°æ®æ›´æ–°å®Œå…¨æŒ‡å—

## ç›®å½•
1. [æ•°æ®æ›´æ–°æ¶æ„æ¦‚è¿°](#æ•°æ®æ›´æ–°æ¶æ„æ¦‚è¿°)
2. [æ•°æ®æºé…ç½®](#æ•°æ®æºé…ç½®)
3. [è‡ªåŠ¨æ›´æ–°æœºåˆ¶](#è‡ªåŠ¨æ›´æ–°æœºåˆ¶)
4. [æ‰‹åŠ¨æ•°æ®æ›´æ–°](#æ‰‹åŠ¨æ•°æ®æ›´æ–°)
5. [æ•°æ®æ¸…æ´—æµç¨‹](#æ•°æ®æ¸…æ´—æµç¨‹)
6. [æ•°æ®åº“å­˜å‚¨ç»“æ„](#æ•°æ®åº“å­˜å‚¨ç»“æ„)
7. [ç›‘æ§å’Œæ•…éšœæ’é™¤](#ç›‘æ§å’Œæ•…éšœæ’é™¤)
8. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## æ•°æ®æ›´æ–°æ¶æ„æ¦‚è¿°

PandaFactorçš„æ•°æ®æ›´æ–°ç³»ç»Ÿé‡‡ç”¨åˆ†å±‚æ¶æ„ï¼Œæ”¯æŒå¤šæ•°æ®æºå’Œè‡ªåŠ¨è°ƒåº¦æœºåˆ¶ã€‚

### ç³»ç»Ÿæ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æ•°æ®æºå±‚      â”‚    â”‚   è°ƒåº¦å±‚        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tushare        â”‚    â”‚ APScheduler     â”‚
â”‚ RiceQuant      â”‚â—„â”€â”€â–ºâ”‚ Cronè§¦å‘å™¨      â”‚
â”‚ è¿…æŠ•(XTQuant)  â”‚    â”‚ BackgroundTasks â”‚
â”‚ QMT            â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ Wind           â”‚             â”‚
â”‚ Choice         â”‚             â–¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚              â”‚   ä¸šåŠ¡é€»è¾‘å±‚    â”‚
         â–¼              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ æ•°æ®æ¸…æ´—å™¨      â”‚
â”‚   å­˜å‚¨å±‚        â”‚    â”‚ å› å­è®¡ç®—å™¨      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”‚ å…ƒæ•°æ®ç®¡ç†å™¨    â”‚
â”‚ MongoDB         â”‚    â”‚ è¿›åº¦ç›‘æ§å™¨      â”‚
â”‚ stocksé›†åˆ      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ stock_marketé›†åˆâ”‚             â”‚
â”‚ factor_baseé›†åˆ  â”‚             â–¼
â”‚ factorsé›†åˆ     â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ logsé›†åˆ        â”‚    â”‚   APIæ¥å£å±‚     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                       â”‚ REST API        â”‚
                       â”‚ Webç•Œé¢         â”‚
                       â”‚ è¿›åº¦æŸ¥è¯¢        â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶è¯´æ˜

1. **panda_data_hub**: æ•°æ®æ›´æ–°æ ¸å¿ƒæ¨¡å—
2. **è°ƒåº¦å™¨**: APSchedulerå®ç°å®šæ—¶ä»»åŠ¡
3. **æ¸…æ´—å™¨**: å„ç§æ•°æ®æºçš„ä¸“ç”¨æ¸…æ´—å™¨
4. **æ•°æ®åº“**: MongoDBå­˜å‚¨æ¸…æ´—åçš„æ•°æ®
5. **APIæœåŠ¡**: FastAPIæä¾›ç®¡ç†å’Œç›‘æ§æ¥å£

## æ•°æ®æºé…ç½®

### 1. æ”¯æŒçš„æ•°æ®æº

| æ•°æ®å•† | æ”¯æŒçŠ¶æ€ | éœ€è¦é…ç½® | ç‰¹æ®Šè¯´æ˜ |
|--------|----------|----------|----------|
| Tushare | âœ… å·²ä¸Šçº¿ | `TS_TOKEN` | å…è´¹ç‰ˆæœ‰é™åˆ¶ |
| RiceQuant | âœ… å·²ä¸Šçº¿ | `MUSER`, `MPASSWORD` | éœ€è¦è´¦å· |
| è¿…æŠ•(XTQuant) | âœ… å·²ä¸Šçº¿ | `XT_TOKEN` | ä¸æ”¯æŒmacOS |
| å¤©å‹¤(Tqsdk) | ğŸš§ æµ‹è¯•ä¸­ | `TQSDK_USERNAME`, `TQSDK_PASSWORD` | - |
| QMT | ğŸš§ æµ‹è¯•ä¸­ | QMTé…ç½® | - |
| Wind | ğŸ”§ å¯¹æ¥ä¸­ | Windé…ç½® | - |
| Choice | ğŸ”§ å¯¹æ¥ä¸­ | Choiceé…ç½® | - |

### 2. é…ç½®æ–‡ä»¶è¯¦è§£

ç¼–è¾‘ `panda_common/config.yaml`ï¼š

```yaml
# æ•°æ®æºé€‰æ‹© (tushare/ricequant/xtquant/tqsdk)
DATASOURCE: "tushare"
DATAHUBSOURCE: "tushare"  # æ•°æ®æ¸…æ´—ä¸“ç”¨æ•°æ®æº

# Tushareé…ç½®
TS_TOKEN: "ä½ çš„tushare_token"

# RiceQuanté…ç½®
MUSER: "your_ricequant_username"
MPASSWORD: "your_ricequant_password"

# è¿…æŠ•é…ç½®
XT_TOKEN: "your_xt_token"

# æ•°æ®æ›´æ–°æ—¶é—´é…ç½®
UPDATE_TIME: '20:00'              # é€šç”¨æ›´æ–°æ—¶é—´
STOCKS_UPDATE_TIME: "20:00"       # è‚¡ç¥¨æ•°æ®æ›´æ–°æ—¶é—´
FACTOR_UPDATE_TIME: "20:30"       # å› å­æ•°æ®æ›´æ–°æ—¶é—´

# æ•°æ®æ¸…æ´—æ—¶é—´èŒƒå›´
HUB_START_DATE: 20170101          # å†å²æ•°æ®å¼€å§‹æ—¥æœŸ
HUB_END_DATE: 20250321            # å†å²æ•°æ®ç»“æŸæ—¥æœŸ
```

### 3. æ•°æ®æºåˆ‡æ¢

```python
# è¿è¡Œæ—¶åˆ‡æ¢æ•°æ®æº
import os

# æ–¹æ³•1ï¼šé€šè¿‡ç¯å¢ƒå˜é‡
os.environ['DATASOURCE'] = 'ricequant'
os.environ['MUSER'] = 'your_username'
os.environ['MPASSWORD'] = 'your_password'

# æ–¹æ³•2ï¼šä¿®æ”¹é…ç½®æ–‡ä»¶åé‡å¯
# ç¼–è¾‘ panda_common/config.yaml
# é‡å¯æœåŠ¡ä½¿é…ç½®ç”Ÿæ•ˆ
```

## è‡ªåŠ¨æ›´æ–°æœºåˆ¶

### 1. è‡ªåŠ¨æ›´æ–°è°ƒåº¦å™¨

PandaFactorä½¿ç”¨åŒé‡è°ƒåº¦å™¨åˆ†åˆ«å¤„ç†è‚¡ç¥¨æ•°æ®å’Œå› å­æ•°æ®ï¼š

```python
# å¯åŠ¨è‡ªåŠ¨æ›´æ–°æœåŠ¡
uv run python -m panda_data_hub._main_auto_
```

#### è‚¡ç¥¨æ•°æ®è°ƒåº¦å™¨ (DataScheduler)

```python
# é…ç½®çš„æ‰§è¡Œæ—¶é—´ï¼šé»˜è®¤æ¯æ—¥20:00
STOCKS_UPDATE_TIME: "20:00"

# æ‰§è¡Œæµç¨‹ï¼š
1. æ£€æŸ¥æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
2. æ¸…æ´—è‚¡ç¥¨å…ƒæ•°æ® (stocksé›†åˆ)
3. æ¸…æ´—å¸‚åœºè¡Œæƒ…æ•°æ® (stock_marketé›†åˆ)
4. æ›´æ–°æŒ‡æ•°æˆåˆ†è‚¡ä¿¡æ¯
```

#### å› å­æ•°æ®è°ƒåº¦å™¨ (FactorCleanerScheduler)

```python
# é…ç½®çš„æ‰§è¡Œæ—¶é—´ï¼šé»˜è®¤æ¯æ—¥20:30
FACTOR_UPDATE_TIME: "20:30"

# æ‰§è¡Œæµç¨‹ï¼š
1. åŸºäºå½“æ—¥å¸‚åœºæ•°æ®
2. è®¡ç®—åŸºç¡€å› å­æ•°æ®
3. è®¡ç®—æ‰©å±•å› å­æ•°æ®
4. å­˜å‚¨åˆ°factor_baseé›†åˆ
```

### 2. è°ƒåº¦å™¨è¯¦ç»†é…ç½®

```python
# panda_data_hub/task/data_scheduler.py
class DataScheduler:
    def schedule_data(self):
        # è§£æé…ç½®çš„æ—¶é—´
        time = self.config["STOCKS_UPDATE_TIME"]  # "20:00"
        hour, minute = time.split(":")

        # åˆ›å»ºCronè§¦å‘å™¨
        trigger = CronTrigger(
            minute=minute,      # åˆ†é’Ÿ
            hour=hour,          # å°æ—¶
            day='*',           # æ¯æ—¥
            month='*',         # æ¯æœˆ
            day_of_week='*'    # æ¯å‘¨
        )

        # æ·»åŠ å®šæ—¶ä»»åŠ¡
        self.scheduler.add_job(
            self._process_data,
            trigger=trigger,
            id=f"data_{datetime.now().strftime('%Y%m%d')}",
            replace_existing=True
        )
```

### 3. é«˜çº§è°ƒåº¦é…ç½®

```python
# è‡ªå®šä¹‰è°ƒåº¦æ—¶é—´
STOCKS_UPDATE_TIME: "19:30"    # å·¥ä½œæ—¥19:30æ›´æ–°
FACTOR_UPDATE_TIME: "22:00"     # å·¥ä½œæ—¥22:00æ›´æ–°

# å‘¨æœ«ä¸æ‰§è¡Œè°ƒåº¦ (éœ€è¦ä»£ç ä¿®æ”¹)
def is_weekday():
    return datetime.now().weekday() < 5  # 0-4ä¸ºå‘¨ä¸€åˆ°å‘¨äº”

# åœ¨_process_dataä¸­æ·»åŠ æ£€æŸ¥
def _process_data(self):
    if not is_weekday():
        logger.info("å‘¨æœ«è·³è¿‡æ•°æ®æ›´æ–°")
        return
    # æ‰§è¡Œæ•°æ®æ¸…æ´—...
```

### 4. è°ƒåº¦å™¨ç›‘æ§

```python
# æŸ¥çœ‹è°ƒåº¦å™¨çŠ¶æ€
import requests

# APIç«¯ç‚¹ (éœ€è¦å¯åŠ¨panda_data_hubæœåŠ¡)
response = requests.get("http://localhost:8222/datahub/api/v1/get_progress")
print(response.json())

# æ£€æŸ¥ä»»åŠ¡æ˜¯å¦æ­£åœ¨è¿è¡Œ
scheduler.get_job(job_id="data_20250327")
```

## æ‰‹åŠ¨æ•°æ®æ›´æ–°

### 1. å¯åŠ¨æ•°æ®æ›´æ–°æœåŠ¡

```bash
# å¯åŠ¨æ•°æ®æ›´æ–°APIæœåŠ¡
uv run python -m panda_data_hub._main_clean_

# æœåŠ¡å°†åœ¨ http://localhost:8222 å¯åŠ¨
```

### 2. Webç•Œé¢æ“ä½œ

è®¿é—® `http://localhost:8222/docs` æŸ¥çœ‹APIæ–‡æ¡£ï¼Œæˆ–ä½¿ç”¨å‰ç«¯ç•Œé¢ï¼š
- `http://localhost:8080/factor/#/datahubsource` - æ•°æ®æºé…ç½®
- `http://localhost:8080/factor/#/datahublist` - æ•°æ®åˆ—è¡¨æŸ¥çœ‹
- `http://localhost:8080/factor/#/datahubdataclean` - è‚¡ç¥¨æ•°æ®æ¸…æ´—
- `http://localhost:8080/factor/#/datahubFactorClean` - å› å­æ•°æ®æ¸…æ´—

### 3. APIæ¥å£è°ƒç”¨

#### è‚¡ç¥¨æ•°æ®æ¸…æ´—

```python
import requests

# æ¸…æ´—æŒ‡å®šæ—¥æœŸèŒƒå›´çš„è‚¡ç¥¨æ•°æ®
base_url = "http://localhost:8222/datahub/api/v1"

# å¯åŠ¨è‚¡ç¥¨æ•°æ®æ¸…æ´—
response = requests.get(
    f"{base_url}/upsert_stockmarket_final",
    params={
        "start_date": "20240101",
        "end_date": "20240320"
    }
)

# æ£€æŸ¥è¿›åº¦
response = requests.get(f"{base_url}/get_progress")
print(response.json())
```

#### å› å­æ•°æ®æ¸…æ´—

```python
# å¯åŠ¨å› å­æ•°æ®æ¸…æ´—
response = requests.get(
    f"{base_url}/upsert_factor_final",
    params={
        "start_date": "20240101",
        "end_date": "20240320"
    }
)

# æ£€æŸ¥å› å­æ¸…æ´—è¿›åº¦
response = requests.get(f"{base_url}/get_progress_factor")
print(response.json())
```

### 4. å‘½ä»¤è¡Œæ‰‹åŠ¨æ›´æ–°

```python
# ç›´æ¥è°ƒç”¨æ¸…æ´—å™¨ (é«˜çº§ç”¨æ³•)
from panda_data_hub.data.tushare_stock_market_cleaner import TSStockMarketCleaner
from panda_common.config import get_config

# è·å–é…ç½®
config = get_config()

# åˆ›å»ºæ¸…æ´—å™¨å®ä¾‹
cleaner = TSStockMarketCleaner(config)

# æ‰§è¡Œå½“æ—¥æ•°æ®æ¸…æ´—
cleaner.stock_market_clean_daily()

# æ‰§è¡Œå†å²æ•°æ®æ¸…æ´—
cleaner.clean_meta_market_data("20240320")
```

### 5. æ‰¹é‡å†å²æ•°æ®æ›´æ–°

```python
# æ‰¹é‡æ›´æ–°å†å²æ•°æ®
from datetime import datetime, timedelta

def batch_update(start_date, end_date, batch_size=30):
    current_date = datetime.strptime(start_date, "%Y%m%d")
    end_date_obj = datetime.strptime(end_date, "%Y%m%d")

    while current_date <= end_date_obj:
        batch_end = min(current_date + timedelta(days=batch_size-1), end_date_obj)

        # è°ƒç”¨APIè¿›è¡Œæ‰¹é‡æ›´æ–°
        response = requests.get(
            f"{base_url}/upsert_stockmarket_final",
            params={
                "start_date": current_date.strftime("%Y%m%d"),
                "end_date": batch_end.strftime("%Y%m%d")
            }
        )

        print(f"Started batch: {current_date.strftime('%Y%m%d')} - {batch_end.strftime('%Y%m%d')}")

        # ç§»åŠ¨åˆ°ä¸‹ä¸€æ‰¹æ¬¡
        current_date = batch_end + timedelta(days=1)

        # ç­‰å¾…ä¸€æ®µæ—¶é—´é¿å…APIé™åˆ¶
        time.sleep(10)

# ä½¿ç”¨ç¤ºä¾‹
batch_update("20240101", "20240320", batch_size=20)
```

## æ•°æ®æ¸…æ´—æµç¨‹

### 1. è‚¡ç¥¨æ•°æ®æ¸…æ´—æµç¨‹

#### Tushareæ•°æ®æºæ¸…æ´—æµç¨‹

```python
# å®Œæ•´çš„è‚¡ç¥¨æ•°æ®æ¸…æ´—æµç¨‹
class TSStockMarketCleaner:
    def stock_market_clean_daily(self):
        # 1. åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¥
        date_str = datetime.now().strftime("%Y%m%d")
        if not self.is_trading_day(date_str):
            logger.info(f"è·³è¿‡éäº¤æ˜“æ—¥: {date_str}")
            return

        # 2. è·å–åŸºç¡€è¡Œæƒ…æ•°æ®
        price_data = self.pro.query('daily', trade_date=date_str)

        # 3. è·å–æŒ‡æ•°æˆåˆ†è‚¡ä¿¡æ¯
        hs_300 = self.pro.query('index_weight', index_code='399300.SZ',
                                start_date=mid_date, end_date=last_date)
        zz_500 = self.pro.query('index_weight', index_code='000905.SH',
                                start_date=mid_date, end_date=last_date)
        zz_1000 = self.pro.query('index_weight', index_code='000852.SH',
                                 start_date=mid_date, end_date=last_date)

        # 4. æ•°æ®å¤„ç†å’Œæ¸…æ´—
        # - æ·»åŠ è‚¡ç¥¨åç§°
        # - è®¡ç®—æ¶¨è·Œåœä»·æ ¼
        # - æ·»åŠ æŒ‡æ•°æˆåˆ†è‚¡æ ‡è¯†
        # - æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç æ ¼å¼

        # 5. æ•°æ®å…¥åº“
        upsert_operations = []
        for record in price_data.to_dict('records'):
            upsert_operations.append(UpdateOne(
                {'date': record['date'], 'symbol': record['symbol']},
                {'$set': record},
                upsert=True
            ))

        self.db_handler.mongo_client[db_name]['stock_market'].bulk_write(upsert_operations)
```

#### æ•°æ®æ¸…æ´—è¯¦ç»†æ­¥éª¤

```python
def clean_meta_market_data(self, date_str):
    """è¯¦ç»†çš„è‚¡ç¥¨å¸‚åœºæ•°æ®æ¸…æ´—æµç¨‹"""

    # Step 1: è·å–åŸºç¡€æ•°æ®
    price_data = self.pro.query('daily', trade_date=date_str)

    # Step 2: æ•°æ®é¢„å¤„ç†
    price_data.reset_index(drop=False, inplace=True)
    price_data['index_component'] = None

    # Step 3: è·å–æŒ‡æ•°æˆåˆ†è‚¡
    mid_date, last_date = self.get_previous_month_dates(date)
    indices = {
        'hs300': self.pro.query('index_weight', index_code='399300.SZ',
                               start_date=mid_date, end_date=last_date),
        'zz500': self.pro.query('index_weight', index_code='000905.SH',
                               start_date=mid_date, end_date=last_date),
        'zz1000': self.pro.query('index_weight', index_code='000852.SH',
                                start_date=mid_date, end_date=last_date)
    }

    # Step 4: å¤„ç†æ¯åªè‚¡ç¥¨çš„æ•°æ®
    for idx, row in price_data.iterrows():
        # æ ‡è®°æŒ‡æ•°æˆåˆ†è‚¡
        component = self.clean_index_components(
            data_symbol=row['ts_code'],
            date=date,
            **indices
        )
        price_data.at[idx, 'index_component'] = component

        # æ·»åŠ è‚¡ç¥¨åç§°
        price_data.at[idx, 'name'] = self.clean_stock_name(row['ts_code'])

    # Step 5: æ•°æ®è½¬æ¢å’Œæ¸…æ´—
    price_data = self.transform_price_data(price_data)

    # Step 6: è®¡ç®—è¡ç”Ÿæ•°æ®
    price_data = self.calculate_derived_fields(price_data)

    # Step 7: æ•°æ®éªŒè¯
    price_data = self.validate_data(price_data)

    # Step 8: å…¥åº“
    self.upsert_to_database(price_data)
```

### 2. å› å­æ•°æ®æ¸…æ´—æµç¨‹

```python
class TSFactorCleaner:
    def clean_daily_factor(self):
        """å› å­æ•°æ®æ¸…æ´—æµç¨‹"""

        # 1. è·å–å½“æ—¥å¸‚åœºæ•°æ®ä½œä¸ºåŸºç¡€
        date = datetime.now().strftime('%Y%m%d')
        market_data = self.get_market_data(date)

        if market_data is None or len(market_data) == 0:
            logger.info(f"No market data for {date}")
            return

        # 2. æ•°æ®é¢„å¤„ç†
        data = pd.DataFrame(list(market_data))
        data = self.preprocess_factor_data(data)

        # 3. è·å–å¸‚å€¼å’Œæ¢æ‰‹ç‡æ•°æ®
        logger.info("è·å–å¸‚å€¼å’Œæ¢æ‰‹ç‡æ•°æ®...")
        basic_data = self.pro.query('daily_basic',
                                   trade_date=date,
                                   fields=['ts_code','turnover_rate','total_mv'])

        # 4. è·å–æˆäº¤é¢æ•°æ®
        logger.info("è·å–æˆäº¤é¢æ•°æ®...")
        amount_data = self.pro.query("daily",
                                    trade_date=date,
                                    fields=['ts_code', 'amount'])

        # 5. æ•°æ®åˆå¹¶å’Œå¤„ç†
        factor_data = self.merge_factor_sources(data, basic_data, amount_data)

        # 6. æ•°æ®æ ‡å‡†åŒ–
        factor_data = self.normalize_factor_data(factor_data)

        # 7. å…¥åº“
        self.upsert_factor_data(factor_data)
```

### 3. æ•°æ®è´¨é‡æ§åˆ¶

```python
class DataQualityController:
    def validate_market_data(self, data):
        """å¸‚åœºæ•°æ®è´¨é‡æ£€æŸ¥"""

        # 1. åŸºç¡€æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        required_fields = ['open', 'high', 'low', 'close', 'volume']
        for field in required_fields:
            if field not in data.columns:
                raise ValueError(f"Missing required field: {field}")

        # 2. ä»·æ ¼åˆç†æ€§æ£€æŸ¥
        invalid_prices = (
            (data['high'] < data['low']) |  # æœ€é«˜ä»·ä¸èƒ½ä½äºæœ€ä½ä»·
            (data['close'] > data['high']) |  # æ”¶ç›˜ä»·ä¸èƒ½é«˜äºæœ€é«˜ä»·
            (data['close'] < data['low']) |  # æ”¶ç›˜ä»·ä¸èƒ½ä½äºæœ€ä½ä»·
            (data['open'] > data['high']) |   # å¼€ç›˜ä»·ä¸èƒ½é«˜äºæœ€é«˜ä»·
            (data['open'] < data['low'])      # å¼€ç›˜ä»·ä¸èƒ½ä½äºæœ€ä½ä»·
        )

        if invalid_prices.any():
            logger.warning(f"Found {invalid_prices.sum()} records with invalid price relationships")
            data = data[~invalid_prices]  # ç§»é™¤æ— æ•ˆæ•°æ®

        # 3. æˆäº¤é‡æ£€æŸ¥
        negative_volume = data['volume'] < 0
        if negative_volume.any():
            logger.warning(f"Found {negative_volume.sum()} records with negative volume")
            data.loc[negative_volume, 'volume'] = 0  # ä¿®æ­£ä¸º0

        # 4. æ¶¨è·Œåœæ£€æŸ¥
        extreme_changes = data['pct_chg'].abs() > 20  # æ¶¨è·Œå¹…è¶…è¿‡20%
        if extreme_changes.any():
            logger.info(f"Found {extreme_changes.sum()} records with extreme price changes")

        # 5. å¼‚å¸¸å€¼æ£€æµ‹
        outliers = self.detect_outliers(data)
        if len(outliers) > 0:
            logger.info(f"Detected {len(outliers)} potential outliers")

        return data

    def detect_outliers(self, data, threshold=3):
        """ä½¿ç”¨IQRæ–¹æ³•æ£€æµ‹å¼‚å¸¸å€¼"""
        outliers = []

        for column in ['open', 'high', 'low', 'close', 'volume']:
            Q1 = data[column].quantile(0.25)
            Q3 = data[column].quantile(0.75)
            IQR = Q3 - Q1

            lower_bound = Q1 - threshold * IQR
            upper_bound = Q3 + threshold * IQR

            column_outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
            outliers.extend(column_outliers.index.tolist())

        return list(set(outliers))
```

## æ•°æ®åº“å­˜å‚¨ç»“æ„

### 1. MongoDBé›†åˆç»“æ„

#### stocksé›†åˆ (è‚¡ç¥¨åŸºç¡€ä¿¡æ¯)

```javascript
{
  "_id": ObjectId("..."),
  "symbol": "000001.SZ",        // æ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 
  "name": "å¹³å®‰é“¶è¡Œ",            // è‚¡ç¥¨åç§°
  "expired": false,             // æ˜¯å¦é€€å¸‚
  "created_at": ISODate("..."),
  "updated_at": ISODate("...")
}

// ç´¢å¼•
db.stocks.createIndex({"symbol": 1}, {unique: true})
db.stocks.createIndex({"expired": 1})
```

#### stock_marketé›†åˆ (æ—¥çº¿è¡Œæƒ…æ•°æ®)

```javascript
{
  "_id": ObjectId("..."),
  "date": "20240320",           // äº¤æ˜“æ—¥æœŸ YYYYMMDD
  "symbol": "000001.SZ",       // è‚¡ç¥¨ä»£ç 
  "open": 10.50,               // å¼€ç›˜ä»·
  "high": 10.80,               // æœ€é«˜ä»·
  "low": 10.40,                // æœ€ä½ä»·
  "close": 10.75,              // æ”¶ç›˜ä»·
  "volume": 1000000,           // æˆäº¤é‡(è‚¡)
  "pre_close": 10.60,          // æ˜¨æ”¶ä»·
  "limit_up": 11.66,           // æ¶¨åœä»·
  "limit_down": 9.54,          // è·Œåœä»·
  "index_component": "hs300",  // æŒ‡æ•°æˆåˆ†è‚¡æ ‡è¯†
  "name": "å¹³å®‰é“¶è¡Œ",          // è‚¡ç¥¨åç§°
  "created_at": ISODate("..."),
  "updated_at": ISODate("...")
}

// å¤åˆç´¢å¼•
db.stock_market.createIndex({"date": 1, "symbol": 1}, {unique: true})
db.stock_market.createIndex({"symbol": 1, "date": 1})
db.stock_market.createIndex({"date": 1})
```

#### factor_baseé›†åˆ (åŸºç¡€å› å­æ•°æ®)

```javascript
{
  "_id": ObjectId("..."),
  "date": "20240320",           // äº¤æ˜“æ—¥æœŸ
  "symbol": "000001.SZ",       // è‚¡ç¥¨ä»£ç 
  "open": 10.50,               // å¼€ç›˜ä»·
  "high": 10.80,               // æœ€é«˜ä»·
  "low": 10.40,                // æœ€ä½ä»·
  "close": 10.75,              // æ”¶ç›˜ä»·
  "volume": 1000000,           // æˆäº¤é‡
  "market_cap": 50000000000,   // æ€»å¸‚å€¼(å…ƒ)
  "turnover": 0.05,            // æ¢æ‰‹ç‡
  "amount": 10750000,          // æˆäº¤é¢(å…ƒ)
  "created_at": ISODate("..."),
  "updated_at": ISODate("...")
}

// å¤åˆç´¢å¼•
db.factor_base.createIndex({"date": 1, "symbol": 1}, {unique: true})
db.factor_base.createIndex({"symbol": 1, "date": 1})
```

#### factorsé›†åˆ (è®¡ç®—å› å­æ•°æ®)

```javascript
{
  "_id": ObjectId("..."),
  "date": "20240320",           // äº¤æ˜“æ—¥æœŸ
  "symbol": "000001.SZ",       // è‚¡ç¥¨ä»£ç 
  "factor_name": "momentum_20d", // å› å­åç§°
  "factor_value": 0.125,        // å› å­å€¼
  "user_id": 123,              // ç”¨æˆ·ID(è‡ªå®šä¹‰å› å­)
  "created_at": ISODate("..."),
  "updated_at": ISODate("...")
}

// å¤åˆç´¢å¼•
db.factors.createIndex({"date": 1, "symbol": 1, "factor_name": 1}, {unique: true})
db.factors.createIndex({"factor_name": 1, "date": 1})
db.factors.createIndex({"user_id": 1, "factor_name": 1})
```

### 2. æ•°æ®åº“æ“ä½œå·¥å…·

```python
from pymongo import UpdateOne, DeleteOne
from panda_common.handlers.database_handler import DatabaseHandler

class DatabaseUtils:
    def __init__(self, config):
        self.db_handler = DatabaseHandler(config)
        self.db_name = config["MONGO_DB"]

    def bulk_upsert_stock_market(self, data):
        """æ‰¹é‡æ›´æ–°è‚¡ç¥¨å¸‚åœºæ•°æ®"""
        collection = self.db_handler.mongo_client[self.db_name]['stock_market']

        operations = []
        for record in data.to_dict('records'):
            operations.append(UpdateOne(
                {'date': record['date'], 'symbol': record['symbol']},
                {'$set': record},
                upsert=True
            ))

        if operations:
            result = collection.bulk_write(operations)
            logger.info(f"Upserted {result.upserted_count} stock market records")
            return result

        return None

    def bulk_upsert_factors(self, factor_data, factor_name, user_id=None):
        """æ‰¹é‡æ›´æ–°å› å­æ•°æ®"""
        collection = self.db_handler.mongo_client[self.db_name]['factors']

        operations = []
        for (date, symbol), value in factor_data.items():
            record = {
                'date': date,
                'symbol': symbol,
                'factor_name': factor_name,
                'factor_value': value,
                'updated_at': datetime.now()
            }
            if user_id:
                record['user_id'] = user_id

            operations.append(UpdateOne(
                {
                    'date': date,
                    'symbol': symbol,
                    'factor_name': factor_name
                },
                {'$set': record},
                upsert=True
            ))

        if operations:
            result = collection.bulk_write(operations)
            logger.info(f"Upserted {result.upserted_count} factor records")
            return result

        return None

    def clean_stale_data(self, days_to_keep=365):
        """æ¸…ç†è¿‡æœŸæ•°æ®"""
        cutoff_date = (datetime.now() - timedelta(days=days_to_keep)).strftime("%Y%m%d")

        # æ¸…ç†æ—§çš„stock_marketæ•°æ®
        result1 = self.db_handler.mongo_delete_many(
            self.db_name,
            'stock_market',
            {'date': {'$lt': cutoff_date}}
        )

        # æ¸…ç†æ—§çš„factor_baseæ•°æ®
        result2 = self.db_handler.mongo_delete_many(
            self.db_name,
            'factor_base',
            {'date': {'$lt': cutoff_date}}
        )

        logger.info(f"Cleaned {result1.deleted_count} stock_market records")
        logger.info(f"Cleaned {result2.deleted_count} factor_base records")
```

### 3. æ•°æ®å¤‡ä»½å’Œæ¢å¤

```python
import json
from datetime import datetime

class DataBackup:
    def __init__(self, db_handler):
        self.db_handler = db_handler

    def export_stocks(self, filename=None):
        """å¯¼å‡ºè‚¡ç¥¨åŸºç¡€ä¿¡æ¯"""
        if filename is None:
            filename = f"stocks_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

        stocks = self.db_handler.mongo_find("panda", "stocks", {})

        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(list(stocks), f, ensure_ascii=False, indent=2, default=str)

        logger.info(f"Exported {len(stocks)} stocks to {filename}")
        return filename

    def export_market_data(self, start_date, end_date, filename=None):
        """å¯¼å‡ºæŒ‡å®šæ—¥æœŸèŒƒå›´çš„è¡Œæƒ…æ•°æ®"""
        if filename is None:
            filename = f"market_data_{start_date}_{end_date}.json"

        query = {
            'date': {'$gte': start_date, '$lte': end_date}
        }
        market_data = self.db_handler.mongo_find("panda", "stock_market", query)

        # åˆ†æ‰¹å¯¼å‡ºé¿å…å†…å­˜æº¢å‡º
        batch_size = 10000
        with open(filename, 'w', encoding='utf-8') as f:
            f.write('[\n')
            batch = []
            for i, record in enumerate(market_data):
                batch.append(record)
                if len(batch) >= batch_size:
                    json.dump(batch, f, ensure_ascii=False, indent=2, default=str)
                    batch = []
                    if i < batch_size:  # ä¸æ˜¯æœ€åä¸€æ‰¹
                        f.write(',\n')

            if batch:  # æœ€åä¸€æ‰¹
                json.dump(batch, f, ensure_ascii=False, indent=2, default=str)
            f.write('\n]')

        logger.info(f"Exported market data to {filename}")
        return filename

    def restore_from_backup(self, backup_file, collection_name):
        """ä»å¤‡ä»½æ–‡ä»¶æ¢å¤æ•°æ®"""
        with open(backup_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        if isinstance(data, list):
            operations = []
            for record in data:
                # æ ¹æ®é›†åˆç±»å‹æ„å»ºæŸ¥è¯¢æ¡ä»¶
                if collection_name == 'stocks':
                    filter_query = {'symbol': record['symbol']}
                elif collection_name in ['stock_market', 'factor_base']:
                    filter_query = {
                        'date': record['date'],
                        'symbol': record['symbol']
                    }
                else:
                    filter_query = {'_id': record.get('_id')}

                operations.append(UpdateOne(
                    filter_query,
                    {'$set': record},
                    upsert=True
                ))

            if operations:
                result = self.db_handler.mongo_client["panda"][collection_name].bulk_write(operations)
                logger.info(f"Restored {result.upserted_count} records to {collection_name}")
                return result

        return None
```

## ç›‘æ§å’Œæ•…éšœæ’é™¤

### 1. æ—¥å¿—ç›‘æ§ç³»ç»Ÿ

```python
import logging
from datetime import datetime, timedelta

class DataUpdateMonitor:
    def __init__(self, db_handler):
        self.db_handler = db_handler
        self.logger = logging.getLogger("DataUpdateMonitor")

    def check_daily_update_status(self, target_date=None):
        """æ£€æŸ¥æŒ‡å®šæ—¥æœŸçš„æ•°æ®æ›´æ–°çŠ¶æ€"""
        if target_date is None:
            target_date = datetime.now().strftime("%Y%m%d")

        # æ£€æŸ¥è‚¡ç¥¨å¸‚åœºæ•°æ®
        market_count = self.db_handler.mongo_count(
            "panda", "stock_market",
            {"date": target_date}
        )

        # æ£€æŸ¥å› å­åŸºç¡€æ•°æ®
        factor_count = self.db_handler.mongo_count(
            "panda", "factor_base",
            {"date": target_date}
        )

        # æ£€æŸ¥è‚¡ç¥¨æ€»æ•°
        total_stocks = self.db_handler.mongo_count("panda", "stocks", {"expired": False})

        status = {
            "date": target_date,
            "market_data_records": market_count,
            "factor_data_records": factor_count,
            "total_stocks": total_stocks,
            "market_coverage": market_count / total_stocks if total_stocks > 0 else 0,
            "factor_coverage": factor_count / total_stocks if total_stocks > 0 else 0,
            "status": "success" if market_count > 0 else "failed"
        }

        self.logger.info(f"Data update status for {target_date}: {status}")
        return status

    def check_data_quality(self, date):
        """æ£€æŸ¥æ•°æ®è´¨é‡"""
        market_data = list(self.db_handler.mongo_find(
            "panda", "stock_market",
            {"date": date}
        ))

        if not market_data:
            return {"status": "no_data"}

        df = pd.DataFrame(market_data)

        quality_report = {
            "date": date,
            "total_records": len(df),
            "null_values": df.isnull().sum().to_dict(),
            "negative_volume": (df['volume'] < 0).sum(),
            "invalid_prices": (
                (df['high'] < df['low']) |
                (df['close'] > df['high']) |
                (df['close'] < df['low'])
            ).sum(),
            "extreme_changes": (df['pct_chg'].abs() > 20).sum() if 'pct_chg' in df.columns else 0
        }

        return quality_report

    def generate_weekly_report(self):
        """ç”Ÿæˆå‘¨åº¦æ•°æ®æ›´æ–°æŠ¥å‘Š"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=7)

        date_range = pd.date_range(start=start_date, end=end_date, freq='D')
        trading_days = []

        for date in date_range:
            date_str = date.strftime("%Y%m%d")
            # è¿™é‡Œéœ€è¦å®é™…çš„äº¤æ˜“æ—¥å†åˆ¤æ–­é€»è¾‘
            if self.is_trading_day(date_str):
                trading_days.append(date_str)

        reports = []
        for date in trading_days:
            status = self.check_daily_update_status(date)
            quality = self.check_data_quality(date)
            reports.append({
                "date": date,
                "update_status": status,
                "data_quality": quality
            })

        return reports

    def alert_on_failure(self, date):
        """æ•°æ®æ›´æ–°å¤±è´¥å‘Šè­¦"""
        status = self.check_daily_update_status(date)

        if status["status"] == "failed":
            # å‘é€å‘Šè­¦é‚®ä»¶/çŸ­ä¿¡/é’‰é’‰æ¶ˆæ¯
            alert_message = f"""
            æ•°æ®æ›´æ–°å¤±è´¥å‘Šè­¦
            æ—¥æœŸ: {date}
            é¢„æœŸè‚¡ç¥¨æ•°: {status['total_stocks']}
            å®é™…æ›´æ–°æ•°: {status['market_data_records']}
            è¦†ç›–ç‡: {status['market_coverage']:.2%}
            è¯·åŠæ—¶æ£€æŸ¥æ•°æ®æ›´æ–°æœåŠ¡ï¼
            """

            # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„å‘Šè­¦ç³»ç»Ÿ
            self.logger.error(alert_message)

            # å‘é€åˆ°ç›‘æ§ç³»ç»Ÿ
            self.send_to_monitoring_system(alert_message)
```

### 2. æ€§èƒ½ç›‘æ§

```python
import time
import psutil
from functools import wraps

class PerformanceMonitor:
    def __init__(self):
        self.logger = logging.getLogger("PerformanceMonitor")

    def monitor_data_cleaning_performance(self, func):
        """è£…é¥°å™¨ï¼šç›‘æ§æ•°æ®æ¸…æ´—æ€§èƒ½"""
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

            try:
                result = func(*args, **kwargs)

                end_time = time.time()
                end_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB

                performance_data = {
                    "function": func.__name__,
                    "execution_time": end_time - start_time,
                    "memory_usage": end_memory - start_memory,
                    "status": "success"
                }

                self.logger.info(f"Performance: {performance_data}")
                return result

            except Exception as e:
                end_time = time.time()

                performance_data = {
                    "function": func.__name__,
                    "execution_time": end_time - start_time,
                    "status": "failed",
                    "error": str(e)
                }

                self.logger.error(f"Performance: {performance_data}")
                raise

        return wrapper

    def monitor_database_performance(self, operation_name):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼šç›‘æ§æ•°æ®åº“æ“ä½œæ€§èƒ½"""
        class DatabasePerformanceMonitor:
            def __init__(self, parent, operation_name):
                self.parent = parent
                self.operation_name = operation_name
                self.start_time = None

            def __enter__(self):
                self.start_time = time.time()
                return self

            def __exit__(self, exc_type, exc_val, exc_tb):
                end_time = time.time()
                execution_time = end_time - self.start_time

                performance_data = {
                    "operation": self.operation_name,
                    "execution_time": execution_time,
                    "status": "success" if exc_type is None else "failed"
                }

                if exc_type is not None:
                    performance_data["error"] = str(exc_val)

                self.parent.logger.info(f"DB Performance: {performance_data}")

        return DatabasePerformanceMonitor(self, operation_name)
```

### 3. å¸¸è§é—®é¢˜è¯Šæ–­

```python
class DataUpdateTroubleshooter:
    def __init__(self, config, db_handler):
        self.config = config
        self.db_handler = db_handler
        self.logger = logging.getLogger("Troubleshooter")

    def diagnose_update_failure(self, date):
        """è¯Šæ–­æ•°æ®æ›´æ–°å¤±è´¥åŸå› """
        diagnosis = {
            "date": date,
            "issues": [],
            "recommendations": []
        }

        # 1. æ£€æŸ¥äº¤æ˜“æ—¥çŠ¶æ€
        if not self.is_trading_day(date):
            diagnosis["issues"].append(f"{date} ä¸æ˜¯äº¤æ˜“æ—¥")
            diagnosis["recommendations"].append("è·³è¿‡éäº¤æ˜“æ—¥çš„æ•°æ®æ›´æ–°")
            return diagnosis

        # 2. æ£€æŸ¥APIé…ç½®
        if self.config['DATAHUBSOURCE'] == 'tushare':
            if not self.config.get('TS_TOKEN'):
                diagnosis["issues"].append("Tushare Token æœªé…ç½®")
                diagnosis["recommendations"].append("é…ç½®æœ‰æ•ˆçš„ Tushare Token")

        # 3. æ£€æŸ¥æ•°æ®åº“è¿æ¥
        try:
            self.db_handler.mongo_client.admin.command('ping')
        except Exception as e:
            diagnosis["issues"].append(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            diagnosis["recommendations"].append("æ£€æŸ¥MongoDBæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ")

        # 4. æ£€æŸ¥æ•°æ®æºAPIé™åˆ¶
        if self.config['DATAHUBSOURCE'] == 'tushare':
            # æ£€æŸ¥ä»Šæ—¥APIè°ƒç”¨æ¬¡æ•°
            api_calls_today = self.get_tushare_api_calls_today()
            if api_calls_today > 20000:  # å‡è®¾æ¯æ—¥é™åˆ¶
                diagnosis["issues"].append(f"Tushare APIè°ƒç”¨æ¬¡æ•°è¿‡å¤š: {api_calls_today}")
                diagnosis["recommendations"].append("ç­‰å¾…APIé™åˆ¶é‡ç½®æˆ–å‡çº§è´¦æˆ·")

        # 5. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        market_count = self.db_handler.mongo_count("panda", "stock_market", {"date": date})
        expected_count = self.db_handler.mongo_count("panda", "stocks", {"expired": False})

        if market_count < expected_count * 0.9:  # è¦†ç›–ç‡ä½äº90%
            diagnosis["issues"].append(f"æ•°æ®ä¸å®Œæ•´: {market_count}/{expected_count}")
            diagnosis["recommendations"].append("é‡æ–°æ‰§è¡Œæ•°æ®æ¸…æ´—ä»»åŠ¡")

        return diagnosis

    def auto_fix_common_issues(self, date):
        """è‡ªåŠ¨ä¿®å¤å¸¸è§é—®é¢˜"""
        fixes_applied = []

        # 1. ä¿®å¤é‡å¤æ•°æ®
        duplicates = self.find_duplicate_records(date)
        if duplicates:
            self.remove_duplicates(duplicates)
            fixes_applied.append(f"åˆ é™¤äº† {len(duplicates)} æ¡é‡å¤è®°å½•")

        # 2. ä¿®å¤é”™è¯¯çš„ä»·æ ¼å…³ç³»
        invalid_prices = self.find_invalid_price_records(date)
        if invalid_prices:
            self.fix_invalid_prices(invalid_prices)
            fixes_applied.append(f"ä¿®å¤äº† {len(invalid_prices)} æ¡ä»·æ ¼å¼‚å¸¸è®°å½•")

        # 3. è¡¥å……ç¼ºå¤±çš„è‚¡ç¥¨åç§°
        missing_names = self.find_missing_stock_names(date)
        if missing_names:
            self.supplement_stock_names(missing_names)
            fixes_applied.append(f"è¡¥å……äº† {len(missing_names)} åªè‚¡ç¥¨çš„åç§°")

        return fixes_applied

    def find_duplicate_records(self, date):
        """æŸ¥æ‰¾é‡å¤è®°å½•"""
        pipeline = [
            {"$match": {"date": date}},
            {"$group": {
                "_id": {"date": "$date", "symbol": "$symbol"},
                "count": {"$sum": 1},
                "docs": {"$push": "$_id"}
            }},
            {"$match": {"count": {"$gt": 1}}}
        ]

        duplicates = list(self.db_handler.mongo_client["panda"]["stock_market"].aggregate(pipeline))
        return duplicates

    def find_invalid_price_records(self, date):
        """æŸ¥æ‰¾ä»·æ ¼å…³ç³»å¼‚å¸¸çš„è®°å½•"""
        query = {
            "date": date,
            "$or": [
                {"high": {"$lt": {"$ifNull": ["$low", 0]}}},
                {"close": {"$gt": {"$ifNull": ["$high", float('inf')]}}},
                {"close": {"$lt": {"$ifNull": ["$low", 0]}}},
                {"open": {"$gt": {"$ifNull": ["$high", float('inf')]}}},
                {"open": {"$lt": {"$ifNull": ["$low", 0]}}}
            ]
        }

        return list(self.db_handler.mongo_find("panda", "stock_market", query))
```

## æœ€ä½³å®è·µ

### 1. æ•°æ®æ›´æ–°ç­–ç•¥

#### å¢é‡æ›´æ–°ç­–ç•¥

```python
class IncrementalUpdateStrategy:
    """å¢é‡æ›´æ–°ç­–ç•¥ï¼Œåªæ›´æ–°ç¼ºå¤±æˆ–é”™è¯¯çš„æ•°æ®"""

    def __init__(self, config, db_handler):
        self.config = config
        self.db_handler = db_handler

    def get_missing_dates(self, start_date, end_date):
        """è·å–ç¼ºå¤±æ•°æ®çš„æ—¥æœŸ"""
        date_range = pd.date_range(start=start_date, end=end_date, freq='D')
        existing_dates = set()

        # ä»æ•°æ®åº“æŸ¥è¯¢å·²å­˜åœ¨çš„æ—¥æœŸ
        pipeline = [
            {"$match": {"date": {"$gte": start_date, "$lte": end_date}}},
            {"$group": {"_id": "$date", "count": {"$sum": 1}}},
            {"$sort": {"_id": 1}}
        ]

        results = list(self.db_handler.mongo_client["panda"]["stock_market"].aggregate(pipeline))
        expected_daily_count = 4000  # é¢„æœŸæ¯æ—¥è‚¡ç¥¨æ•°é‡

        for result in results:
            if result["count"] >= expected_daily_count * 0.9:  # è¦†ç›–ç‡è¶…è¿‡90%
                existing_dates.add(result["_id"])

        missing_dates = []
        for date in date_range:
            date_str = date.strftime("%Y%m%d")
            if date_str not in existing_dates and self.is_trading_day(date_str):
                missing_dates.append(date_str)

        return missing_dates

    def smart_update(self, start_date, end_date):
        """æ™ºèƒ½æ›´æ–°ï¼šåªæ›´æ–°ç¼ºå¤±çš„æ•°æ®"""
        missing_dates = self.get_missing_dates(start_date, end_date)

        if not missing_dates:
            logger.info("æ‰€æœ‰æ•°æ®éƒ½æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ›´æ–°")
            return

        logger.info(f"å‘ç° {len(missing_dates)} ä¸ªäº¤æ˜“æ—¥æ•°æ®ç¼ºå¤±ï¼Œå¼€å§‹å¢é‡æ›´æ–°")

        for date in missing_dates:
            try:
                self.update_single_date(date)
                logger.info(f"æˆåŠŸæ›´æ–° {date} çš„æ•°æ®")
            except Exception as e:
                logger.error(f"æ›´æ–° {date} æ•°æ®å¤±è´¥: {str(e)}")
                continue
```

#### æ•°æ®éªŒè¯ç­–ç•¥

```python
class DataValidationStrategy:
    """æ•°æ®éªŒè¯ç­–ç•¥"""

    def __init__(self, db_handler):
        self.db_handler = db_handler

    def validate_batch_data(self, data_batch):
        """æ‰¹é‡æ•°æ®éªŒè¯"""
        validation_results = {
            "total_records": len(data_batch),
            "valid_records": 0,
            "invalid_records": 0,
            "warnings": [],
            "errors": []
        }

        valid_records = []

        for record in data_batch:
            record_valid = True
            record_warnings = []

            # åŸºç¡€å­—æ®µæ£€æŸ¥
            required_fields = ['date', 'symbol', 'open', 'high', 'low', 'close', 'volume']
            for field in required_fields:
                if field not in record or record[field] is None:
                    validation_results["errors"].append(f"è®°å½• {record.get('symbol', 'unknown')} ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                    record_valid = False
                    break

            if not record_valid:
                validation_results["invalid_records"] += 1
                continue

            # ä»·æ ¼åˆç†æ€§æ£€æŸ¥
            if record['high'] < record['low']:
                validation_results["errors"].append(f"{record['symbol']} æœ€é«˜ä»·ä½äºæœ€ä½ä»·")
                record_valid = False

            if not (record['low'] <= record['close'] <= record['high']):
                validation_results["errors"].append(f"{record['symbol']} æ”¶ç›˜ä»·è¶…å‡ºå½“æ—¥ä»·æ ¼åŒºé—´")
                record_valid = False

            if not (record['low'] <= record['open'] <= record['high']):
                validation_results["errors"].append(f"{record['symbol']} å¼€ç›˜ä»·è¶…å‡ºå½“æ—¥ä»·æ ¼åŒºé—´")
                record_valid = False

            # æˆäº¤é‡æ£€æŸ¥
            if record['volume'] < 0:
                validation_results["warnings"].append(f"{record['symbol']} æˆäº¤é‡ä¸ºè´Ÿæ•°")
                record['volume'] = 0  # è‡ªåŠ¨ä¿®æ­£
                record_warnings.append("æˆäº¤é‡å·²ä¿®æ­£ä¸º0")

            # æ¶¨è·Œå¹…æ£€æŸ¥
            if 'pre_close' in record and record['pre_close'] > 0:
                pct_change = (record['close'] - record['pre_close']) / record['pre_close'] * 100
                if abs(pct_change) > 20:  # å‡è®¾æ¶¨è·Œå¹…é™åˆ¶ä¸º20%
                    validation_results["warnings"].append(f"{record['symbol']} æ¶¨è·Œå¹…å¼‚å¸¸: {pct_change:.2f}%")

            if record_valid:
                valid_records.append(record)
                validation_results["valid_records"] += 1

                if record_warnings:
                    validation_results["warnings"].extend(record_warnings)
            else:
                validation_results["invalid_records"] += 1

        return valid_records, validation_results
```

### 2. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### æ•°æ®åº“æ“ä½œä¼˜åŒ–

```python
class DatabaseOptimization:
    """æ•°æ®åº“æ“ä½œä¼˜åŒ–"""

    def __init__(self, db_handler):
        self.db_handler = db_handler

    def optimize_bulk_operations(self, data, collection_name, batch_size=1000):
        """ä¼˜åŒ–æ‰¹é‡æ“ä½œ"""
        from pymongo import UpdateOne

        collection = self.db_handler.mongo_client["panda"][collection_name]
        total_records = len(data)
        processed = 0

        while processed < total_records:
            batch = data[processed:processed + batch_size]

            operations = []
            for record in batch:
                if collection_name == 'stock_market':
                    filter_query = {'date': record['date'], 'symbol': record['symbol']}
                elif collection_name == 'factors':
                    filter_query = {
                        'date': record['date'],
                        'symbol': record['symbol'],
                        'factor_name': record['factor_name']
                    }
                else:
                    filter_query = {'_id': record.get('_id')}

                operations.append(UpdateOne(
                    filter_query,
                    {'$set': record},
                    upsert=True
                ))

            if operations:
                try:
                    result = collection.bulk_write(operations, ordered=False)
                    processed += len(batch)
                    logger.info(f"Processed {processed}/{total_records} records in {collection_name}")

                    # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡è½½
                    time.sleep(0.1)

                except Exception as e:
                    logger.error(f"Bulk operation failed: {str(e)}")
                    # å¯ä»¥é€‰æ‹©é‡è¯•æˆ–è®°å½•å¤±è´¥è®°å½•
                    break

    def create_optimal_indexes(self):
        """åˆ›å»ºä¼˜åŒ–ç´¢å¼•"""
        db = self.db_handler.mongo_client["panda"]

        # stock_marketé›†åˆç´¢å¼•
        db.stock_market.create_index([("date", 1), ("symbol", 1)], unique=True)
        db.stock_market.create_index([("symbol", 1), ("date", 1)])
        db.stock_market.create_index([("date", 1)])
        db.stock_market.create_index([("symbol", 1)])

        # factorsé›†åˆç´¢å¼•
        db.factors.create_index([("date", 1), ("symbol", 1), ("factor_name", 1)], unique=True)
        db.factors.create_index([("factor_name", 1), ("date", 1)])
        db.factors.create_index([("user_id", 1), ("factor_name", 1)])

        logger.info("Created optimal indexes for better query performance")
```

#### å†…å­˜ä¼˜åŒ–ç­–ç•¥

```python
class MemoryOptimization:
    """å†…å­˜ä½¿ç”¨ä¼˜åŒ–"""

    def process_large_dataset(self, data_generator, batch_processor):
        """å¤„ç†å¤§æ•°æ®é›†çš„å†…å­˜ä¼˜åŒ–æ–¹æ³•"""
        batch_size = 1000
        batch = []

        for record in data_generator:
            batch.append(record)

            if len(batch) >= batch_size:
                # å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_processor(batch)

                # æ¸…ç†å†…å­˜
                batch.clear()

                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()

        # å¤„ç†æœ€åä¸€æ‰¹
        if batch:
            batch_processor(batch)

    def optimize_dataframe_memory(self, df):
        """ä¼˜åŒ–DataFrameå†…å­˜ä½¿ç”¨"""
        original_memory = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB

        # ä¼˜åŒ–æ•°å€¼ç±»å‹
        for col in df.select_dtypes(include=['int64']).columns:
            df[col] = pd.to_numeric(df[col], downcast='integer')

        for col in df.select_dtypes(include=['float64']).columns:
            df[col] = pd.to_numeric(df[col], downcast='float')

        # ä¼˜åŒ–å­—ç¬¦ä¸²ç±»å‹
        for col in df.select_dtypes(include=['object']).columns:
            if df[col].dtype == 'object':
                df[col] = df[col].astype('category')

        optimized_memory = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
        reduction = (original_memory - optimized_memory) / original_memory * 100

        logger.info(f"Memory optimized: {original_memory:.2f}MB -> {optimized_memory:.2f}MB ({reduction:.1f}% reduction)")

        return df
```

### 3. å®¹é”™å’Œæ¢å¤ç­–ç•¥

#### è‡ªåŠ¨é‡è¯•æœºåˆ¶

```python
import time
from functools import wraps
from random import uniform

class RetryMechanism:
    """è‡ªåŠ¨é‡è¯•æœºåˆ¶"""

    @staticmethod
    def retry_on_failure(max_retries=3, base_delay=1, max_delay=60, backoff_factor=2):
        """é‡è¯•è£…é¥°å™¨"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                last_exception = None

                for attempt in range(max_retries + 1):
                    try:
                        return func(*args, **kwargs)

                    except Exception as e:
                        last_exception = e

                        if attempt == max_retries:
                            logger.error(f"Function {func.__name__} failed after {max_retries} retries: {str(e)}")
                            raise

                        # è®¡ç®—å»¶è¿Ÿæ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ + éšæœºæŠ–åŠ¨ï¼‰
                        delay = min(base_delay * (backoff_factor ** attempt) + uniform(0, 1), max_delay)
                        logger.warning(f"Function {func.__name__} failed (attempt {attempt + 1}/{max_retries + 1}), retrying in {delay:.2f}s: {str(e)}")
                        time.sleep(delay)

                raise last_exception

            return wrapper
        return decorator

# ä½¿ç”¨ç¤ºä¾‹
@RetryMechanism.retry_on_failure(max_retries=3, base_delay=2)
def fetch_market_data(date, data_source):
    """è·å–å¸‚åœºæ•°æ®ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
    # å®é™…çš„æ•°æ®è·å–é€»è¾‘
    pass
```

#### æ•°æ®æ¢å¤æœºåˆ¶

```python
class DataRecovery:
    """æ•°æ®æ¢å¤æœºåˆ¶"""

    def __init__(self, config, db_handler):
        self.config = config
        self.db_handler = db_handler

    def create_recovery_point(self, date):
        """åˆ›å»ºæ¢å¤ç‚¹"""
        recovery_point = {
            "date": date,
            "timestamp": datetime.now(),
            "market_data_count": self.db_handler.mongo_count("panda", "stock_market", {"date": date}),
            "factor_data_count": self.db_handler.mongo_count("panda", "factor_base", {"date": date}),
            "custom_factors_count": self.db_handler.mongo_count("panda", "factors", {"date": date})
        }

        # ä¿å­˜æ¢å¤ç‚¹ä¿¡æ¯
        self.db_handler.mongo_insert("panda", "recovery_points", recovery_point)

        logger.info(f"Created recovery point for {date}")
        return recovery_point

    def rollback_to_recovery_point(self, date):
        """å›æ»šåˆ°æ¢å¤ç‚¹"""
        recovery_point = self.db_handler.mongo_find_one("panda", "recovery_points", {"date": date})

        if not recovery_point:
            raise ValueError(f"No recovery point found for {date}")

        # åˆ é™¤æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰æ•°æ®ï¼ˆé€šå¸¸ç”¨äºæ•°æ®æŸåæ—¶é‡æ–°å¤„ç†ï¼‰
        delete_result = self.db_handler.mongo_delete_many("panda", "stock_market", {"date": date})
        delete_result += self.db_handler.mongo_delete_many("panda", "factor_base", {"date": date})
        delete_result += self.db_handler.mongo_delete_many("panda", "factors", {"date": date})

        logger.info(f"Rollback completed for {date}, deleted {delete_result} records")

        return recovery_point

    def detect_data_corruption(self, date):
        """æ£€æµ‹æ•°æ®æŸå"""
        corruption_indicators = []

        # æ£€æŸ¥æ•°æ®é‡å¼‚å¸¸
        market_count = self.db_handler.mongo_count("panda", "stock_market", {"date": date})
        expected_count = 4000  # é¢„æœŸè‚¡ç¥¨æ•°é‡

        if market_count < expected_count * 0.5:  # æ•°æ®é‡è¿‡å°‘
            corruption_indicators.append(f"Insufficient market data: {market_count}/{expected_count}")

        if market_count > expected_count * 1.5:  # æ•°æ®é‡è¿‡å¤šï¼ˆå¯èƒ½æœ‰é‡å¤ï¼‰
            corruption_indicators.append(f"Excessive market data: {market_count}/{expected_count}")

        # æ£€æŸ¥ä»·æ ¼å¼‚å¸¸
        invalid_prices = self.db_handler.mongo_count("panda", "stock_market", {
            "date": date,
            "$or": [
                {"high": {"$lt": {"$ifNull": ["$low", 0]}}},
                {"$expr": {"$gt": ["$close", "$high"]}},
                {"$expr": {"$lt": ["$close", "$low"]}}
            ]
        })

        if invalid_prices > 0:
            corruption_indicators.append(f"Found {invalid_prices} records with invalid price relationships")

        return corruption_indicators
```

### 4. å®‰å…¨ç­–ç•¥

#### APIè®¿é—®æ§åˆ¶

```python
import hashlib
import secrets
from datetime import datetime, timedelta

class APIAccessControl:
    """APIè®¿é—®æ§åˆ¶"""

    def __init__(self, db_handler):
        self.db_handler = db_handler

    def generate_api_key(self, user_id, permissions=None):
        """ç”ŸæˆAPIå¯†é’¥"""
        if permissions is None:
            permissions = ["read_data", "write_factor"]

        api_key = secrets.token_urlsafe(32)
        api_secret = secrets.token_urlsafe(32)

        key_record = {
            "user_id": user_id,
            "api_key": api_key,
            "api_secret": api_secret,
            "permissions": permissions,
            "created_at": datetime.now(),
            "expires_at": datetime.now() + timedelta(days=365),
            "is_active": True,
            "rate_limit": 1000  # æ¯å°æ—¶è¯·æ±‚é™åˆ¶
        }

        self.db_handler.mongo_insert("panda", "api_keys", key_record)

        return {
            "api_key": api_key,
            "api_secret": api_secret,
            "permissions": permissions
        }

    def validate_api_request(self, api_key, api_secret, required_permission):
        """éªŒè¯APIè¯·æ±‚"""
        key_record = self.db_handler.mongo_find_one("panda", "api_keys", {
            "api_key": api_key,
            "is_active": True
        })

        if not key_record:
            return False, "Invalid API key"

        # éªŒè¯å¯†é’¥
        if key_record["api_secret"] != api_secret:
            return False, "Invalid API secret"

        # æ£€æŸ¥æƒé™
        if required_permission not in key_record["permissions"]:
            return False, f"Permission denied: {required_permission} required"

        # æ£€æŸ¥è¿‡æœŸæ—¶é—´
        if datetime.now() > key_record["expires_at"]:
            return False, "API key expired"

        # æ£€æŸ¥é€Ÿç‡é™åˆ¶
        current_hour = datetime.now().replace(minute=0, second=0, microsecond=0)
        request_count = self.db_handler.mongo_count("panda", "api_requests", {
            "api_key": api_key,
            "timestamp": {"$gte": current_hour}
        })

        if request_count >= key_record["rate_limit"]:
            return False, "Rate limit exceeded"

        # è®°å½•è¯·æ±‚
        self.db_handler.mongo_insert("panda", "api_requests", {
            "api_key": api_key,
            "permission": required_permission,
            "timestamp": datetime.now()
        })

        return True, "Access granted"
```

#### æ•°æ®å¤‡ä»½ç­–ç•¥

```python
import schedule
import threading

class BackupStrategy:
    """æ•°æ®å¤‡ä»½ç­–ç•¥"""

    def __init__(self, db_handler, backup_path="./backups"):
        self.db_handler = db_handler
        self.backup_path = backup_path
        self.backup_thread = None

    def daily_backup(self):
        """æ¯æ—¥å¤‡ä»½ç­–ç•¥"""
        today = datetime.now().strftime("%Y%m%d")

        # å¤‡ä»½å…³é”®æ•°æ®
        collections_to_backup = ["stocks", "stock_market", "factor_base", "factors"]

        for collection in collections_to_backup:
            backup_file = f"{self.backup_path}/{collection}_{today}.json"
            self.export_collection_to_json(collection, backup_file)

        # æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™30å¤©ï¼‰
        self.cleanup_old_backups(days_to_keep=30)

        logger.info(f"Daily backup completed for {today}")

    def export_collection_to_json(self, collection_name, filename):
        """å¯¼å‡ºé›†åˆåˆ°JSONæ–‡ä»¶"""
        try:
            # ä½¿ç”¨mongodumpçš„Pythonå®ç°
            data = list(self.db_handler.mongo_find("panda", collection_name, {}))

            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2, default=str)

            logger.info(f"Exported {len(data)} records from {collection_name} to {filename}")

        except Exception as e:
            logger.error(f"Failed to export {collection_name}: {str(e)}")

    def schedule_backup_tasks(self):
        """è°ƒåº¦å¤‡ä»½ä»»åŠ¡"""
        # æ¯æ—¥å‡Œæ™¨2ç‚¹æ‰§è¡Œå¤‡ä»½
        schedule.every().day.at("02:00").do(self.daily_backup)

        # å¯åŠ¨è°ƒåº¦çº¿ç¨‹
        def run_scheduler():
            while True:
                schedule.run_pending()
                time.sleep(60)

        self.backup_thread = threading.Thread(target=run_scheduler, daemon=True)
        self.backup_thread.start()

        logger.info("Backup scheduler started")
```

---

é€šè¿‡æœ¬æŒ‡å—ï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿï¼š

1. **ç†è§£æ•°æ®æ›´æ–°æ¶æ„**: æŒæ¡PandaFactorçš„æ•°æ®æ›´æ–°æœºåˆ¶å’Œç»„ä»¶
2. **é…ç½®æ•°æ®æº**: æ­£ç¡®é…ç½®å„ç§æ•°æ®æºå’ŒAPIå¯†é’¥
3. **è®¾ç½®è‡ªåŠ¨æ›´æ–°**: é…ç½®å®šæ—¶ä»»åŠ¡å’Œè°ƒåº¦å™¨
4. **æ‰§è¡Œæ‰‹åŠ¨æ›´æ–°**: ä½¿ç”¨APIæˆ–å‘½ä»¤è¡Œæ‰‹åŠ¨æ›´æ–°æ•°æ®
5. **ç›‘æ§æ•°æ®è´¨é‡**: å®æ–½ç›‘æ§å’Œæ•…éšœæ’é™¤æœºåˆ¶
6. **ä¼˜åŒ–æ€§èƒ½**: åº”ç”¨æ€§èƒ½ä¼˜åŒ–å’Œæœ€ä½³å®è·µ
7. **ç¡®ä¿æ•°æ®å®‰å…¨**: å®æ–½å¤‡ä»½å’Œæ¢å¤ç­–ç•¥

è¿™å¥—æ•°æ®æ›´æ–°æœºåˆ¶ç¡®ä¿äº†PandaFactorå¹³å°çš„æ•°æ®æ—¶æ•ˆæ€§ã€å‡†ç¡®æ€§å’Œå¯é æ€§ï¼Œä¸ºé‡åŒ–å› å­ç ”ç©¶æä¾›äº†åšå®çš„æ•°æ®åŸºç¡€ã€‚